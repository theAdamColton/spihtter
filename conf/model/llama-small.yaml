model_type: llama
hidden_size: 128
intermediate_size: 512
num_attention_heads: 8
num_hidden_layers: 8
vocab_size: 32
use_cache: true
rope_theta: 500000
max_position_embeddings: 1024
